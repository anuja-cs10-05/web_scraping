import time
import pickle
import json
import csv
import random
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

# Amazon credentials
USERNAME = "anujamishra0808@gmail.com"
PASSWORD = "Anuja@123"

# URLs of best sellers categories
CATEGORY_URLS = [
    "https://www.amazon.in/gp/bestsellers/kitchen/ref=zg_bs_nav_kitchen_0",
    "https://www.amazon.in/gp/bestsellers/shoes/ref=zg_bs_nav_shoes_0",
    # Add more category URLs as needed
]

# Initialize WebDriver
def init_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--enable-logging")
    options.add_argument("--v=1")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver

# Login to Amazon
def amazon_login(driver):
    driver.get("https://www.amazon.in/ap/signin")
    try:
        print("Navigated to login page.")
        
        email_field = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "ap_email"))
        )
        email_field.send_keys(USERNAME)
        email_field.send_keys(Keys.RETURN)
        print("Email entered.")

        time.sleep(random.uniform(1, 2))  # Simulate human behavior

        password_field = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "ap_password"))
        )
        password_field.send_keys(PASSWORD)
        password_field.send_keys(Keys.RETURN)
        print("Password entered.")

        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "nav-logo-sprites"))
        )
        print("Login successful.")

    except TimeoutException:
        print("Login failed. Check the structure or presence of CAPTCHA/OTP.")
        driver.quit()
        exit()

# Save cookies after manual login
def save_cookies(driver):
    with open("cookies.pkl", "wb") as cookie_file:
        pickle.dump(driver.get_cookies(), cookie_file)
    print("Cookies saved.")

# Load cookies for subsequent sessions
def load_cookies(driver):
    driver.get("https://www.amazon.in")
    with open("cookies.pkl", "rb") as cookie_file:
        cookies = pickle.load(cookie_file)
        for cookie in cookies:
            driver.add_cookie(cookie)
    driver.refresh()
    print("Cookies loaded and page refreshed.")

# Scrape product details from a category
def scrape_category(driver, url):
    driver.get(url)
    time.sleep(3)  # Let the page load

    products = []
    try:
        for _ in range(3):  # Adjust for pagination to get more products
            product_elements = driver.find_elements(By.CSS_SELECTOR, "div.zg-grid-general-faceout")
            
            print(f"Found {len(product_elements)} products on page.")  # Debug print
            
            for product in product_elements:
                try:
                    product_name = product.find_element(By.CSS_SELECTOR, "span.zg-text-center-align a div").text
                    product_price = product.find_element(By.CSS_SELECTOR, "span.p13n-sc-price").text
                    discount_element = product.find_elements(By.CSS_SELECTOR, "span.a-text-price")
                    discount = "Unavailable" if not discount_element else discount_element[-1].text
                    best_seller_rating = product.find_element(By.CSS_SELECTOR, "span.a-icon-alt").text

                    products.append({
                        "Product Name": product_name,
                        "Product Price": product_price,
                        "Discount": discount,
                        "Best Seller Rating": best_seller_rating,
                        "Category": url.split('/')[-1],  # Use the URL to infer the category
                    })
                except NoSuchElementException:
                    continue

            # Navigate to the next page
            next_button = driver.find_elements(By.CSS_SELECTOR, "li.a-last a")
            if next_button:
                next_button[0].click()
                time.sleep(3)
            else:
                print("No next page found.")
                break
    except Exception as e:
        print(f"Error scraping category {url}: {e}")

    return products

# Save data to CSV
def save_to_csv(data, filename):
    if not data:
        print("No data to save.")
        return  # Avoid saving empty data
    
    keys = data[0].keys()
    with open(filename, "w", newline="", encoding="utf-8") as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(data)
    print(f"Data saved to {filename}.")

# Save data to JSON
def save_to_json(data, filename):
    with open(filename, "w", encoding="utf-8") as output_file:
        json.dump(data, output_file, ensure_ascii=False, indent=4)
    print(f"Data saved to {filename}.")

# Main function to run the scraper
def main():
    driver = init_driver()
    try:
        # Use cookies if available, otherwise log in
        try:
            load_cookies(driver)
        except FileNotFoundError:
            amazon_login(driver)
            save_cookies(driver)

        all_products = []
        for category_url in CATEGORY_URLS:
            products = scrape_category(driver, category_url)
            all_products.extend(products)

        save_to_csv(all_products, "amazon_best_sellers.csv")
        save_to_json(all_products, "amazon_best_sellers.json")
    finally:
        driver.quit()

if __name__ == "__main__":
    main()
